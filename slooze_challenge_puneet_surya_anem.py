# -*- coding: utf-8 -*-
"""Slooze_challenge_Puneet_Surya_Anem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xb9LjCZR4sSurVR6GP5ZJUUlOwt2ehad

#Step 1: Data Acquisition and Initial Exploration

The very first thing we need to do in any data science project is to get our hands on the data and understand what we're working with. For this challenge, we're using a dataset from Kaggle, and to ensure everyone can easily run this analysis, we'll store the data in Google Drive and then mount that Drive into our Colab environment. This avoids any issues with API keys or local file paths.

##1.1 Mounting Google Drive
First, we'll connect our Google Colab environment to Google Drive. This allows our notebook to access files stored in your Drive.
"""

# Import the 'drive' module from google.colab. This module provides functions to mount Google Drive.
from google.colab import drive

# The 'drive.mount()' function is called to mount your Google Drive at the specified path,
# '/content/drive'. When you run this cell, a prompt will appear asking you to
# authorize Google Colab to access your Google Drive. You'll need to click the link,
# select your Google account, and grant the necessary permissions.
# This step is crucial for the notebook to find the dataset files.
drive.mount('/content/drive')

"""##1.2 Defining Data Path and Loading Datasets
Once the Drive is mounted, we need to tell our notebook exactly where the data files are located within your Google Drive. After that, we'll use the pandas library to load each CSV file into its own DataFrame. Pandas is a fantastic library for data manipulation and analysis in Python.

**Important Note for Proctor:**

Before running this cell, please ensure you have downloaded the dataset from Kaggle (the slooze-challenge.zip file), unzipped it, and placed all six .csv files into a specific folder in your Google Drive. For instance, I've created a folder named slooze_challenge_data directly under "My Drive". You might need to adjust the data_path variable below to match the exact location where you've stored these CSV files in your own Google Drive.
"""

import pandas as pd # We import pandas, commonly aliased as 'pd', which is indispensable for data handling.

# Define the path to the folder containing your CSV files in Google Drive.
# It's good practice to make this a variable so it's easy to change if your folder structure differs.
# In my case, I've placed the files in a folder named 'slooze_challenge_data' directly under 'My Drive'.
data_path = '/content/drive/MyDrive/slooze_challenge_data/'

# Now, let's load each of the six CSV files into separate Pandas DataFrames.
# We're giving them descriptive names to easily understand their content later on.

# This DataFrame will hold information about purchase prices in December 2017.
df_purchase_prices_dec_2017 = pd.read_csv(data_path + '2017PurchasePricesDec.csv')

# This DataFrame contains the beginning inventory levels as of December 31, 2016.
df_beginning_inventory_2016 = pd.read_csv(data_path + 'BegInvFINAL12312016.csv')

# This DataFrame contains the ending inventory levels as of December 31, 2016.
df_ending_inventory_2016 = pd.read_csv(data_path + 'EndInvFINAL12312016.csv')

# This DataFrame likely details individual invoice purchases made up to December 31, 2016.
df_invoice_purchases_2016 = pd.read_csv(data_path + 'InvoicePurchases12312016.csv')

# This DataFrame should contain a summarized view of purchases made up to December 31, 2016.
df_purchases_final_2016 = pd.read_csv(data_path + 'PurchasesFINAL12312016.csv')

# Finally, this DataFrame will contain sales transaction data up to December 31, 2016.
df_sales_final_2016 = pd.read_csv(data_path + 'SalesFINAL12312016.csv')

print("All datasets loaded successfully!")

"""##1.3 Initial Data Exploration - Getting a First Look

After loading the data, the next critical step is to get a feel for what's inside each DataFrame. This involves checking their structure, data types, and looking for any immediate issues like missing values. This "sniff test" helps us plan our data cleaning and preparation steps.

We'll use a few common Pandas methods for this:



*   head(): Shows the first few rows of the DataFrame, giving us a peek at the
actual data.
*   info(): Provides a concise summary of the DataFrame, including the number of entries, number of columns, non-null values per column, and data types (Dtypes). This is incredibly useful for spotting potential data type issues (e.g., numbers stored as strings).

*  describe(): Generates descriptive statistics for numerical columns, such as count, mean, standard deviation, min, max, and quartiles. This helps understand the distribution and range of our numerical data.
*  isnull().sum(): Counts the number of missing (NaN) values in each column. This is vital for deciding how to handle missing data later (e.g., imputation, deletion).



Let's go through each DataFrame one by one.
"""

# --- Exploring df_purchase_prices_dec_2017 ---
print("--- Exploring '2017PurchasePricesDec.csv' (df_purchase_prices_dec_2017) ---")
print("\nHead (First 5 rows):")
print(df_purchase_prices_dec_2017.head())
print("\nInfo (Column types and non-null counts):")
df_purchase_prices_dec_2017.info()
print("\nDescription (Statistical summary of numerical columns):")
print(df_purchase_prices_dec_2017.describe())
print("\nMissing values per column:")
print(df_purchase_prices_dec_2017.isnull().sum())
print("\n" + "="*80 + "\n") # A separator for better readability

# --- Exploring df_beginning_inventory_2016 ---
print("--- Exploring 'BegInvFINAL12312016.csv' (df_beginning_inventory_2016) ---")
print("\nHead (First 5 rows):")
print(df_beginning_inventory_2016.head())
print("\nInfo (Column types and non-null counts):")
df_beginning_inventory_2016.info()
print("\nDescription (Statistical summary of numerical columns):")
print(df_beginning_inventory_2016.describe())
print("\nMissing values per column:")
print(df_beginning_inventory_2016.isnull().sum())
print("\n" + "="*80 + "\n")

# --- Exploring df_ending_inventory_2016 ---
print("--- Exploring 'EndInvFINAL12312016.csv' (df_ending_inventory_2016) ---")
print("\nHead (First 5 rows):")
print(df_ending_inventory_2016.head())
print("\nInfo (Column types and non-null counts):")
df_ending_inventory_2016.info()
print("\nDescription (Statistical summary of numerical columns):")
print(df_ending_inventory_2016.describe())
print("\nMissing values per column:")
print(df_ending_inventory_2016.isnull().sum())
print("\n" + "="*80 + "\n")

# --- Exploring df_invoice_purchases_2016 ---
print("--- Exploring 'InvoicePurchases12312016.csv' (df_invoice_purchases_2016) ---")
print("\nHead (First 5 rows):")
print(df_invoice_purchases_2016.head())
print("\nInfo (Column types and non-null counts):")
df_invoice_purchases_2016.info()
print("\nDescription (Statistical summary of numerical columns):")
print(df_invoice_purchases_2016.describe())
print("\nMissing values per column:")
print(df_invoice_purchases_2016.isnull().sum())
print("\n" + "="*80 + "\n")

# --- Exploring df_purchases_final_2016 ---
print("--- Exploring 'PurchasesFINAL12312016.csv' (df_purchases_final_2016) ---")
print("\nHead (First 5 rows):")
print(df_purchases_final_2016.head())
print("\nInfo (Column types and non-null counts):")
df_purchases_final_2016.info()
print("\nDescription (Statistical summary of numerical columns):")
print(df_purchases_final_2016.describe())
print("\nMissing values per column:")
print(df_purchases_final_2016.isnull().sum())
print("\n" + "="*80 + "\n")

# --- Exploring df_sales_final_2016 ---
print("--- Exploring 'SalesFINAL12312016.csv' (df_sales_final_2016) ---")
print("\nHead (First 5 rows):")
print(df_sales_final_2016.head())
print("\nInfo (Column types and non-null counts):")
df_sales_final_2016.info()
print("\nDescription (Statistical summary of numerical columns):")
print(df_sales_final_2016.describe())
print("\nMissing values per column:")
print(df_sales_final_2016.isnull().sum())
print("\n" + "="*80 + "\n")

"""#Step 2: Data Cleaning and Preparation

##2.1 Date Column Conversion
One of the most common and important cleaning steps is converting date columns from generic 'object' (string) types to proper datetime objects. This allows us to perform time-series analysis, filter by dates, and generally work with time-based data much more effectively. Pandas has a great function for this: pd.to_datetime().
"""

import pandas as pd # Ensure pandas is imported, although it should be from Step 1.

# --- df_beginning_inventory_2016 ---
# Converting 'startDate' to datetime. This enables us to easily filter or group by date later.
print("Converting 'startDate' in df_beginning_inventory_2016 to datetime...")
df_beginning_inventory_2016['startDate'] = pd.to_datetime(df_beginning_inventory_2016['startDate'])
print("Conversion complete for df_beginning_inventory_2016.\n")
print("\n" + "="*80 + "\n")

# --- df_ending_inventory_2016 ---
# Similarly, converting 'endDate' for consistency and usability.
print("Converting 'endDate' in df_ending_inventory_2016 to datetime...")
df_ending_inventory_2016['endDate'] = pd.to_datetime(df_ending_inventory_2016['endDate'])
print("Conversion complete for df_ending_inventory_2016.\n")
print("\n" + "="*80 + "\n")

# --- df_invoice_purchases_2016 ---
# This DataFrame has multiple date columns that are vital for understanding purchase timelines.
print("Converting date columns in df_invoice_purchases_2016 to datetime...")
df_invoice_purchases_2016['InvoiceDate'] = pd.to_datetime(df_invoice_purchases_2016['InvoiceDate'])
df_invoice_purchases_2016['PODate'] = pd.to_datetime(df_invoice_purchases_2016['PODate'])
df_invoice_purchases_2016['PayDate'] = pd.to_datetime(df_invoice_purchases_2016['PayDate'])
print("Conversion complete for df_invoice_purchases_2016.\n")
print("\n" + "="*80 + "\n")

# --- df_purchases_final_2016 ---
# A comprehensive purchase record also needs its dates in proper format.
print("Converting date columns in df_purchases_final_2016 to datetime...")
df_purchases_final_2016['PODate'] = pd.to_datetime(df_purchases_final_2016['PODate'])
df_purchases_final_2016['ReceivingDate'] = pd.to_datetime(df_purchases_final_2016['ReceivingDate'])
df_purchases_final_2016['InvoiceDate'] = pd.to_datetime(df_purchases_final_2016['InvoiceDate'])
df_purchases_final_2016['PayDate'] = pd.to_datetime(df_purchases_final_2016['PayDate'])
print("Conversion complete for df_purchases_final_2016.\n")
print("\n" + "="*80 + "\n")

# --- df_sales_final_2016 ---
# The sales date is critical for demand forecasting, so this is a must-do.
print("Converting 'SalesDate' in df_sales_final_2016 to datetime...")
df_sales_final_2016['SalesDate'] = pd.to_datetime(df_sales_final_2016['SalesDate'])
print("Conversion complete for df_sales_final_2016.\n")
print("\n" + "="*80 + "\n")
print("\nAll specified date columns have been converted to datetime objects.")
print("\n" + "="*80 + "\n")

"""## 2.2 Handling Missing Values

We found missing values in df_purchase_prices_dec_2017, df_ending_inventory_2016, df_invoice_purchases_2016, and df_purchases_final_2016. We'll address each case thoughtfully.

###2.2.1 df_purchase_prices_dec_2017 (Description, Size, Volume)
We observed one missing value in 'Description', 'Size', and 'Volume'. For such a small number of missing records (just one row out of 12,261), the simplest and often safest approach is to drop the row. This prevents potential issues with imputation for categorical fields like 'Description' or 'Size', and numerical 'Volume' without clear context.
"""

# Drop rows where any of the specified columns have missing values.
# 'subset' ensures we only check these specific columns for NaNs.
original_rows = df_purchase_prices_dec_2017.shape[0]
print(f"Original number of rows: {original_rows}")
df_purchase_prices_dec_2017.dropna(subset=['Description', 'Size', 'Volume'], inplace=True)
rows_after_drop = df_purchase_prices_dec_2017.shape[0]
print(f"Dropped {original_rows - rows_after_drop} row(s) from df_purchase_prices_dec_2017 due to missing Description, Size, or Volume.")
print(f"Remaining rows: {rows_after_drop}\n")

"""###2.2.2 df_ending_inventory_2016 (City)
The City column in df_ending_inventory_2016 has 1284 missing values. Dropping these rows would mean losing a substantial amount of inventory data. A better strategy here is to impute the missing city values based on the 'Store' ID. It's highly probable that each store is located in a single, consistent city. We can verify this and then fill in the blanks.
"""

import numpy as np # Importing numpy for np.nan

# --- Refined City Imputation for df_ending_inventory_2016 ---
print("--- City Imputation for df_ending_inventory_2016 ---")

# Step 1: Create a reliable Store-to-City mapping.
# We'll use df_beginning_inventory_2016 because it has no missing cities.
# We group by 'Store' and take the first 'City' encountered for each store.
# This assumes that for any given Store ID, the City is consistent across all entries.
# Let's ensure this assumption holds by checking if there are multiple cities per store in the source data.
# (We already did this check and it showed consistency, but good to be explicit about the source of the map).
store_city_map_reliable = df_beginning_inventory_2016[['Store', 'City']].drop_duplicates().set_index('Store')['City']

# Step 2: Fill missing 'City' values in df_ending_inventory_2016 using this map.
# We'll use the .map() method, which is efficient for this kind of lookup.
# First, extract the 'Store' IDs where 'City' is currently missing in df_ending_inventory_2016.
missing_city_stores = df_ending_inventory_2016[df_ending_inventory_2016['City'].isnull()]['Store']

# Now, map these Store IDs to their corresponding cities from our reliable map.
# This will create a Series of cities for the missing entries.
imputed_cities = missing_city_stores.map(store_city_map_reliable)

# Fill the original DataFrame's 'City' column using .fillna() with the imputed_cities Series.
# The .fillna() method will align on the index, so only the matching missing values will be filled.
df_ending_inventory_2016['City'].fillna(imputed_cities, inplace=True)

# Step 3: Verify if there are still missing 'City' values.
remaining_missing_cities = df_ending_inventory_2016['City'].isnull().sum()

if remaining_missing_cities == 0:
    print("Successfully imputed all missing 'City' values in df_ending_inventory_2016.")
else:
    print(f"Warning: After imputation, {remaining_missing_cities} 'City' values still remain missing in df_ending_inventory_2016.")
    # If there are still missing values, it means some 'Store' IDs in df_ending_inventory_2016
    # did not have a corresponding City in df_beginning_inventory_2016.
    # We might need to decide how to handle these very few cases (e.g., fill with 'Unknown', or drop if negligible).
    # For now, let's fill them with 'Unknown' to avoid further errors and proceed.
    df_ending_inventory_2016['City'].fillna('Unknown', inplace=True)
    print("Filled remaining missing 'City' values with 'Unknown' for completeness.")


print(f"Final missing 'City' values in df_ending_inventory_2016: {df_ending_inventory_2016['City'].isnull().sum()}\n")

# Let's also do a final check of the info for this specific DataFrame to ensure everything is perfect.
print("--- df_ending_inventory_2016 Info (After Refined City Cleaning) ---")
df_ending_inventory_2016.info()
print("\nMissing values:")
print(df_ending_inventory_2016.isnull().sum())
print("\n" + "="*80 + "\n")

"""###2.2.3 df_invoice_purchases_2016 (Approval)
The 'Approval' column has over 90% missing values. This suggests that either approval records were rarely captured in this dataset, or 'NaN' implicitly means 'not approved' or 'approval not applicable'. Given the high percentage, it's unlikely to be useful for detailed analysis without external context. For our purposes, we'll decide to drop this column, as it won't contribute meaningfully to inventory optimization or sales/purchase insights.
"""

# Dropping the 'Approval' column due to an overwhelming number of missing values.
# It's unlikely to provide reliable insights with so much sparsity.
if 'Approval' in df_invoice_purchases_2016.columns:
    df_invoice_purchases_2016.drop('Approval', axis=1, inplace=True)
    print("Dropped 'Approval' column from df_invoice_purchases_2016 due to high missing value count.\n")
else:
    print("'Approval' column not found in df_invoice_purchases_2016 (already dropped or named differently).\n")

"""###2.2.4 df_purchases_final_2016 (Size)
Similar to df_purchase_prices_dec_2017, df_purchases_final_2016 has only 3 missing values in the 'Size' column. Again, for such a small number, dropping these rows is the most straightforward solution, avoiding complex imputation that might introduce inaccuracies.
"""

original_rows_pf = df_purchases_final_2016.shape[0]
print(f"Original nunber of rows are: {original_rows_pf}")
df_purchases_final_2016.dropna(subset=['Size'], inplace=True)
rows_after_drop_pf = df_purchases_final_2016.shape[0]
print(f"Dropped {original_rows_pf - rows_after_drop_pf} row(s) from df_purchases_final_2016 due to missing Size.\n")
print(f"Remaining rows: {rows_after_drop_pf}\n")

"""##2.3 Data Type Correction / Consistency
We noted that 'Volume' in df_purchase_prices_dec_2017 is an 'object' (string) type, but should be numeric. This needs careful handling because Size values are strings like "750mL" and 'Volume' might be derived from these.

Let's re-examine df_purchase_prices_dec_2017['Volume'] to see if it contains non-numeric characters after dropping the NaN row. If it's a mix, we might need to parse it. If it was an 'object' type due to a single non-numeric value or string formatting, then a simple conversion might work after handling that.
"""

# First, let's inspect the unique values and their types in the 'Volume' column
# to understand what kind of conversion is needed.
print("Unique values in 'Volume' column of df_purchase_prices_dec_2017 (before conversion):")
print(df_purchase_prices_dec_2017['Volume'].unique())

# Attempt to convert 'Volume' to numeric. 'errors='coerce'' will turn any non-convertible values into NaN.
# This is a safe way to identify problematic entries.
df_purchase_prices_dec_2017['Volume'] = pd.to_numeric(df_purchase_prices_dec_2017['Volume'], errors='coerce')

# Check if any new NaNs were introduced (meaning there were non-numeric strings).
newly_coerced_nans = df_purchase_prices_dec_2017['Volume'].isnull().sum()
if newly_coerced_nans > 0:
    print(f"\nWarning: {newly_coerced_nans} non-numeric 'Volume' entries were coerced to NaN in df_purchase_prices_dec_2017.")
    # We could inspect these rows: df_purchase_prices_dec_2017[df_purchase_prices_dec_2017['Volume'].isnull()]
    # For simplicity, if these are few, we might drop them or fill with median/mode.
    # Given we just dropped one row, let's assume it was that one problematic entry, or if new NaNs appear,
    # it indicates data quality issues beyond simple conversion.
    # For now, if there are new NaNs, let's check for remaining ones, and assume dropping is best.
    df_purchase_prices_dec_2017.dropna(subset=['Volume'], inplace=True)
    print("Dropped rows with newly coerced NaN 'Volume' values.\n")


# After conversion, we can convert to integer if all values are whole numbers, or keep as float.
# Let's ensure it's integer if no decimals are present.
df_purchase_prices_dec_2017['Volume'] = df_purchase_prices_dec_2017['Volume'].astype(int)
print("Converted 'Volume' column in df_purchase_prices_dec_2017 to integer type.\n")

print("Data type consistency checks and conversions completed.")

"""##2.4 Investigate Zero/Unusual Values
We identified 0.00 values in 'Price' and 'PurchasePrice' in a few dataframes. Zero prices could represent promotional items, sampling, or data errors. Zero purchase prices could mean free goods or errors. Similarly, very low sales prices (0.49) exist.

For now, we will just flag these observations and not remove them unless our analysis explicitly states they are errors or skew results (e.g., calculating average price where 0 would drastically lower it). They might represent legitimate business cases. For the scope of this general cleaning, we'll keep them but be aware.
"""

print("--- Investigating Zero/Unusual Price/PurchasePrice Values ---")

# Check df_purchase_prices_dec_2017 for 0 PurchasePrice
zero_purchase_price_2017 = df_purchase_prices_dec_2017[df_purchase_prices_dec_2017['PurchasePrice'] == 0]
if not zero_purchase_price_2017.empty:
    print(f"df_purchase_prices_dec_2017 has {len(zero_purchase_price_2017)} items with 0 PurchasePrice. Examples:\n{zero_purchase_price_2017.head()}")
else:
    print("No items with 0 PurchasePrice found in df_purchase_prices_dec_2017.")
print("\n" + "="*80 + "\n")

# Check df_beginning_inventory_2016 for 0 Price
zero_beg_inv_price = df_beginning_inventory_2016[df_beginning_inventory_2016['Price'] == 0]
if not zero_beg_inv_price.empty:
    print(f"\ndf_beginning_inventory_2016 has {len(zero_beg_inv_price)} items with 0 Price. Examples:\n{zero_beg_inv_price.head()}")
else:
    print("No items with 0 Price found in df_beginning_inventory_2016.")
print("\n" + "="*80 + "\n")

# Check df_purchases_final_2016 for 0 PurchasePrice and 0 Dollars
zero_purchase_final = df_purchases_final_2016[(df_purchases_final_2016['PurchasePrice'] == 0) | (df_purchases_final_2016['Dollars'] == 0)]
if not zero_purchase_final.empty:
    print(f"\ndf_purchases_final_2016 has {len(zero_purchase_final)} entries with 0 PurchasePrice or 0 Dollars. Examples:\n{zero_purchase_final.head()}")
else:
    print("No entries with 0 PurchasePrice or 0 Dollars found in df_purchases_final_2016.")
print("\n" + "="*80 + "\n")

# Check df_sales_final_2016 for very low SalesDollars/SalesPrice (min is 0.49)
low_sales_price = df_sales_final_2016[df_sales_final_2016['SalesPrice'] < 1.0] # Items under $1
if not low_sales_price.empty:
    print(f"\ndf_sales_final_2016 has {len(low_sales_price)} entries with SalesPrice < $1.0. Examples:\n{low_sales_price.head()}")
else:
    print("No entries with SalesPrice < $1.0 found in df_sales_final_2016.")

print("\n" + "="*80 + "\n")
print("\nInvestigation of zero/unusual price values complete. No values removed at this stage, but noted for future analysis consideration.")
print("\n" + "="*80 + "\n")

"""##2.5 Final Check After Cleaning
It's always a good idea to perform a quick .info() and .isnull().sum() check on all DataFrames after cleaning to confirm our actions had the desired effect.
"""

print("\n" + "="*80)
print("--- Post-Cleaning DataFrames Info Summary ---")
print("="*80 + "\n")

print("--- df_purchase_prices_dec_2017 Info (After Cleaning) ---")
df_purchase_prices_dec_2017.info()
print("\nMissing values:")
print(df_purchase_prices_dec_2017.isnull().sum())
print("\n" + "-"*50 + "\n")

print("--- df_beginning_inventory_2016 Info (After Cleaning) ---")
df_beginning_inventory_2016.info()
print("\nMissing values:")
print(df_beginning_inventory_2016.isnull().sum())
print("\n" + "-"*50 + "\n")

print("--- df_ending_inventory_2016 Info (After Cleaning) ---")
df_ending_inventory_2016.info()
print("\nMissing values:")
print(df_ending_inventory_2016.isnull().sum())
print("\n" + "-"*50 + "\n")

print("--- df_invoice_purchases_2016 Info (After Cleaning) ---")
df_invoice_purchases_2016.info()
print("\nMissing values:")
print(df_invoice_purchases_2016.isnull().sum())
print("\n" + "-"*50 + "\n")

print("--- df_purchases_final_2016 Info (After Cleaning) ---")
df_purchases_final_2016.info()
print("\nMissing values:")
print(df_purchases_final_2016.isnull().sum())
print("\n" + "-"*50 + "\n")

print("--- df_sales_final_2016 Info (After Cleaning) ---")
df_sales_final_2016.info()
print("\nMissing values:")
print(df_sales_final_2016.isnull().sum())
print("\n" + "-"*50 + "\n")

print("\nData cleaning and preparation complete!")



"""# TASK 1: DEMAND FORECASTING

##1.1 Preparing Sales Data for Time Series Analysis
To predict future demand, we first need to define what "demand" means in a time-series context. This usually involves aggregating sales over specific time intervals (like daily, weekly, or monthly) and by the item we want to forecast (e.g., individual products, brands, or overall sales).

Given the dataset is from 2016, and to keep the initial forecasting manageable while still demonstrating the concept, let's aggregate sales by SalesDate and Brand. This will give us the daily sales quantity for each brand, which is a good balance between granularity and computational load.
"""

import pandas as pd
import matplotlib.pyplot as plt # For basic plotting
import seaborn as sns # For enhanced visualizations
from statsmodels.tsa.seasonal import seasonal_decompose # For time series decomposition
from statsmodels.tsa.arima.model import ARIMA # A common time series model
from sklearn.metrics import mean_squared_error # To evaluate our model
import numpy as np # For numerical operations

# Ensure SalesDate is datetime, as confirmed in the previous step.
# If for any reason it wasn't, this line would re-convert it (but it should be already done).
df_sales_final_2016['SalesDate'] = pd.to_datetime(df_sales_final_2016['SalesDate'])

print("--- Preparing Sales Data for Time Series Forecasting ---")

# Aggregating sales data:
# We'll group by 'SalesDate' and 'Brand' to get daily sales quantity per brand.
# We sum 'SalesQuantity' to get the total quantity sold for each brand on each day.
daily_brand_sales = df_sales_final_2016.groupby(['SalesDate', 'Brand'])['SalesQuantity'].sum().reset_index()

# Renaming the aggregated column for clarity.
daily_brand_sales.rename(columns={'SalesQuantity': 'DailySalesQuantity'}, inplace=True)

print("\nAggregated daily sales quantity by Brand:")
print(daily_brand_sales.head())
print(f"\nShape of aggregated data: {daily_brand_sales.shape}")
print(f"Number of unique brands: {daily_brand_sales['Brand'].nunique()}")
print(f"Time period covered: {daily_brand_sales['SalesDate'].min().date()} to {daily_brand_sales['SalesDate'].max().date()}")

# For a first pass at forecasting, let's pick one of the top-selling brands
# to demonstrate the process. This helps in understanding the methodology
# before scaling it up or applying more complex models.
# Let's find a popular brand to focus on for initial modeling.
top_brands = df_sales_final_2016.groupby('Brand')['SalesQuantity'].sum().nlargest(5)
print(f"\nTop 5 brands by total sales quantity:\n{top_brands}")

# Let's select the top-selling brand for our initial forecasting example.
# We'll pick the Brand ID of the top brand.
# For example, if Brand ID 1004 is the top, we'll filter for it.
selected_brand_id = top_brands.index[0] # Get the ID of the highest selling brand
print(f"\nSelected top-selling brand for initial forecasting: Brand ID {selected_brand_id}")

# Filter the aggregated data for the selected brand
brand_sales_ts = daily_brand_sales[daily_brand_sales['Brand'] == selected_brand_id]

# Set 'SalesDate' as the index for time series analysis.
# This makes it easier for time-series specific operations.
brand_sales_ts.set_index('SalesDate', inplace=True)

# We only need the 'DailySalesQuantity' for our time series.
brand_sales_ts = brand_sales_ts[['DailySalesQuantity']]

# It's crucial to ensure our time series has a complete date range for time series models.
# If there are missing dates (days with no sales for this brand), we need to fill them,
# typically with 0 sales.
full_date_range = pd.date_range(start=brand_sales_ts.index.min(),
                                end=brand_sales_ts.index.max(),
                                freq='D')
brand_sales_ts = brand_sales_ts.reindex(full_date_range, fill_value=0)

print(f"\nTime series for Brand {selected_brand_id} (first 5 entries):\n{brand_sales_ts.head()}")
print(f"Time series for Brand {selected_brand_id} (last 5 entries):\n{brand_sales_ts.tail()}")
print(f"Shape of Brand {selected_brand_id} time series: {brand_sales_ts.shape}")

"""##1.2 Visualizing the Time Series
Before jumping into modeling, it's always a good idea to visualize the time series. This helps us spot trends, seasonality, and any irregular patterns.
"""

plt.figure(figsize=(14, 7))
plt.plot(brand_sales_ts.index, brand_sales_ts['DailySalesQuantity'], label=f'Daily Sales for Brand {selected_brand_id}')
plt.title(f'Daily Sales Quantity for Brand {selected_brand_id} (2016)')
plt.xlabel('Date')
plt.ylabel('Sales Quantity')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# Decompose the time series to observe trend, seasonality, and residuals.
# This helps in understanding the underlying components of the data.
# We'll assume an additive model (trend + seasonality + residual) and a daily frequency.
# However, for daily data, simple `seasonal_decompose` might struggle if the period is too long
# or if there's no clear daily pattern. Let's try weekly or monthly for general trend.
# For daily data, a common seasonal period for retail might be 7 (for weekly patterns) or 365 (for yearly).
# Given it's 2016 data, let's try a weekly decomposition if enough data points exist.
# If sales are consistently zero for many days, decomposition might not be ideal.
# Let's first check for non-zero sales count.

if brand_sales_ts['DailySalesQuantity'].sum() > 0: # Ensure there's actual sales data
    try:
        # We need to specify a period for seasonality. For daily data, a weekly pattern (period=7) is common.
        decomposition = seasonal_decompose(brand_sales_ts['DailySalesQuantity'], model='additive', period=7)
        fig = decomposition.plot()
        fig.set_size_inches(12, 8)
        fig.suptitle(f'Time Series Decomposition for Brand {selected_brand_id}', y=1.02)
        plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent title overlap
        plt.show()
    except Exception as e:
        print(f"Could not perform seasonal decomposition, often due to insufficient data points for the period or constant values: {e}")
        print("Skipping decomposition plot for this brand.")
else:
    print(f"No sales data for Brand {selected_brand_id} to decompose (all zeros).")

"""**KEY OBSERVATIONS**

The **daily sales** show high volatility in January 2016, with sales ranging from around 550 to over 1,400 units. There's a dramatic drop around February 1st, after which sales stabilize at much lower levels (around 50-150 units daily).

**Time Series Decomposition (Image 2):**

The decomposition reveals:

Trend: Shows the underlying pattern with a sharp decline around February 1st

Seasonal: Clear weekly seasonality with regular peaks and troughs

Residual: The random fluctuations after removing trend and seasonality

##1.3 Simple Demand Forecasting Model (ARIMA - Example)
For demonstration, we'll use a simple ARIMA model. ARIMA (AutoRegressive Integrated Moving Average) is a popular and robust method for time series forecasting. It models the future values based on past values (autoregressive), past forecast errors (moving average), and differences to make the series stationary (integrated).

Important Considerations for ARIMA:

Stationarity: ARIMA models assume the time series is stationary (mean, variance, and autocorrelation are constant over time). Differencing (d parameter in ARIMA) is used to achieve this.

p, d, q parameters:

p: Order of the AR part (number of lagged observations).

d: Order of the differencing (number of times the raw observations are
differenced).

q: Order of the MA part (number of lagged forecast errors).

Seasonality (SARIMA): If strong seasonality is observed (which is likely in retail), a Seasonal ARIMA (SARIMA) model would be more appropriate. For simplicity here, we'll start with ARIMA, but a real-world application would definitely lean towards SARIMA or Prophet.

Given the time constraint and the objective to demonstrate the approach, we'll choose simple p, d, q parameters for a start. Ideally, these parameters are determined through methods like ACF/PACF plots or auto-ARIMA.
"""

# For demonstration purposes, we'll split the data into training and testing sets.
# We'll use the last 30 days of 2016 for testing and the rest for training.
train_size = len(brand_sales_ts) - 30 # Use roughly the last month for testing
train_data, test_data = brand_sales_ts[0:train_size], brand_sales_ts[train_size:]

print(f"\nTraining data points: {len(train_data)}")
print(f"Testing data points: {len(test_data)}")

# Fit an ARIMA model.
# The parameters (p, d, q) are crucial. For a basic demo, we'll pick some values.
# A common starting point for daily data without obvious complex patterns might be (5,1,0).
# (p=5: 5 past observations, d=1: one differencing to make it stationary, q=0: no moving average part)
# In a real scenario, 'auto_arima' or careful ACF/PACF analysis would determine optimal parameters.
# Given it's sales data, d=1 is a reasonable assumption for stationarity.
order = (7, 1, 0) # Example: Autoregressive(7), Integrated(1), Moving Average(0)

print(f"\nAttempting to fit ARIMA model with order {order} for Brand {selected_brand_id}...")
try:
    # Suppressing warnings that often appear during ARIMA fitting, especially with sparse data.
    import warnings
    warnings.filterwarnings("ignore")
    model = ARIMA(train_data['DailySalesQuantity'], order=order)
    model_fit = model.fit()
    print(model_fit.summary())

    # Make predictions on the test set
    # The 'start' and 'end' parameters define the prediction range.
    # The 'dynamic=False' means in-sample predictions are made. For forecasting, you'd use dynamic=True or specify future dates.
    # Here, we predict over the test period.
    predictions = model_fit.predict(start=len(train_data), end=len(brand_sales_ts)-1, dynamic=False)

    # Convert predictions to a Series with the correct index
    predictions_series = pd.Series(predictions, index=test_data.index)

    # Evaluate the model using Mean Squared Error (MSE)
    rmse = np.sqrt(mean_squared_error(test_data['DailySalesQuantity'], predictions_series))
    print(f'\nRoot Mean Squared Error (RMSE) on test set for Brand {selected_brand_id}: {rmse:.2f}')

    # Plot actual vs. predicted sales
    plt.figure(figsize=(15, 7))
    plt.plot(train_data.index, train_data['DailySalesQuantity'], label='Training Data')
    plt.plot(test_data.index, test_data['DailySalesQuantity'], label='Actual Sales (Test Data)', color='orange')
    plt.plot(predictions_series.index, predictions_series, label='ARIMA Predictions', color='green', linestyle='--')
    plt.title(f'Daily Sales Forecasting for Brand {selected_brand_id} (ARIMA Model)')
    plt.xlabel('Date')
    plt.ylabel('Sales Quantity')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

except Exception as e:
    print(f"\nError fitting ARIMA model for Brand {selected_brand_id}: {e}")
    print("This can happen if the time series is too sparse, constant, or parameters are ill-suited.")
    print("Consider a different model or aggregation level if this error persists for many brands.")

"""Limited Data Window (60 days): The time series for Brand 8111 covers only 60 days (Jan 1 to Feb 29, 2016). This is a very short period for robust time series forecasting, especially for detecting long-term trends or complex seasonal patterns.

Structural Break/Dramatic Drop: Your most critical observation is the "dramatic drop around February 1st." This is a major issue for standard time series models like ARIMA. ARIMA assumes that the underlying patterns (mean, variance, seasonality) remain relatively constant over time. A sharp, sustained drop like this signifies a change in the data-generating process, which an ARIMA model trained before the drop will struggle to predict accurately for the period after the drop.

The model was trained on the high-sales period of January (30 days), and then asked to predict the low-sales period of February (30 days). It naturally overestimates because it learned the January patterns.

Possible Reasons for the Drop:

End of a major promotion/sale.

Initial product launch hype dying down.

Seasonal shift (e.g., end of holiday season).

Out-of-stock situation (product ran out).

Competitor activity or market shift.

Data error (less likely but always a possibility).

Weekly Seasonality: The decomposition correctly identified weekly seasonality, which is common in retail. This is a great finding and suggests that if we had more data and a more stable trend, a SARIMA model (Seasonal ARIMA) with a period=7 would be highly appropriate.

ARIMA Performance (RMSE: 835.13): The RMSE of 835.13 is very high, especially given the actual sales are often in the 50-150 range post-break. This confirms the model's struggle to adapt to the new, lower sales regime. The model is essentially predicting January-level sales for February.

#TASK 2: ABC Analysis

##2.1 Calculate Total Sales Value per Brand
First, we need to aggregate our sales data to find the total SalesDollars for each Brand over the entire period available in df_sales_final_2016.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'Brand' is treated correctly as a categorical or key identifier.
# We'll use the 'Brand' column directly for grouping.

print("--- Performing ABC Analysis ---")
print("\nCalculating total sales value per brand...\n")

# Group by 'Brand' and sum 'SalesDollars' to get the total revenue generated by each brand.
# We're calculating the 'consumption value' for each item (brand in this case).
brand_sales_value = df_sales_final_2016.groupby('Brand')['SalesDollars'].sum().reset_index()

# Rename the aggregated column for clarity, making it explicit that it's the total sales value.
brand_sales_value.rename(columns={'SalesDollars': 'TotalSalesValue'}, inplace=True)

# Sort the brands in descending order of their total sales value.
# This is a crucial step for ABC analysis, as we categorize from highest to lowest value.
brand_sales_value_sorted = brand_sales_value.sort_values(by='TotalSalesValue', ascending=False)

print("Top 10 Brands by Total Sales Value: \n")
print(brand_sales_value_sorted.head(10))
print(f"\nTotal unique brands: {brand_sales_value_sorted.shape[0]}")

"""##2.2 Calculate Cumulative Percentage and Assign ABC Categories
Now, we'll calculate the cumulative percentage of total sales value for each brand. This helps us identify the cut-off points for A, B, and C categories. The standard ABC categories are:

A-items: Up to 80% of the total sales value.
B-items: From 80% to 95% of the total sales value.
C-items: The remaining 5% of the total sales value (from 95% to 100%).
"""

# Calculate the total sales value across all brands.
total_overall_sales_value = brand_sales_value_sorted['TotalSalesValue'].sum()

# Calculate the percentage of total sales value for each brand.
brand_sales_value_sorted['PercentageOfTotalValue'] = (
    (brand_sales_value_sorted['TotalSalesValue'] / total_overall_sales_value) * 100
)

# Calculate the cumulative percentage of sales value.
# This helps us identify the 'A', 'B', and 'C' cut-off points.
brand_sales_value_sorted['CumulativePercentage'] = (
    brand_sales_value_sorted['PercentageOfTotalValue'].cumsum()
)

# Assign ABC categories based on cumulative percentage.
# We'll apply a function row-wise to categorize each brand.
def assign_abc_category(cumulative_percentage):
    if cumulative_percentage <= 80:
        return 'A'
    elif cumulative_percentage <= 95:
        return 'B'
    else:
        return 'C'

brand_sales_value_sorted['ABC_Category'] = brand_sales_value_sorted['CumulativePercentage'].apply(assign_abc_category)

print("\nBrands with their Sales Value, Percentage, Cumulative Percentage, and ABC Category (First 15 rows):")
print(brand_sales_value_sorted.head(15))

print("\nDistribution of Brands by ABC Category:")
print(brand_sales_value_sorted['ABC_Category'].value_counts())
print("\nPercentage distribution of Brands by ABC Category:")
print(brand_sales_value_sorted['ABC_Category'].value_counts(normalize=True) * 100)

"""The ABC analysis provides a framework for differentiated inventory control:

A-Items (High Priority):

Focus: These are the critical few. They demand close monitoring, accurate forecasting (perhaps using more advanced models), and frequent review of inventory levels.

Strategies:

Could include just-in-time (JIT) inventory, very accurate demand forecasting, frequent order placements, tight security, and detailed record-keeping. They should always be in stock.

B-Items (Medium Priority):

Focus: These items are important but don't require the same intense level of control as A-items.

Strategies: Regular monitoring, good forecasting methods (perhaps less rigorous than A-items), and standard inventory control policies.

C-Items (Low Priority):

Focus: These are the "trivial many." While numerous, their individual impact on overall value is small.

Strategies: Simple inventory control, larger order quantities to reduce administrative costs (even if it means higher holding costs), less frequent reviews, and potentially simpler forecasting (e.g., using moving averages). Sometimes, even two-bin systems or visual control are used.

This analysis helps businesses allocate resources effectively, ensuring that the most valuable inventory items receive the attention they deserve, while less critical items are managed efficiently without over-expending effort.

##2.3 Visualizing the ABC Analysis
A Pareto chart is often used to visualize ABC analysis. It shows the individual values in descending order and the cumulative sum, making the A, B, C classifications clear.
"""

plt.figure(figsize=(16, 8))

# Create a bar plot for the individual percentage of total value
ax1 = sns.barplot(x=brand_sales_value_sorted['Brand'].astype(str),
                  y=brand_sales_value_sorted['PercentageOfTotalValue'],
                  color='skyblue',
                  label='Individual Percentage')

# Create a second y-axis for the cumulative percentage line plot
ax2 = ax1.twinx()
sns.lineplot(x=ax1.get_xticks(), # Use numerical positions for line plot x-axis to align with bars
             y=brand_sales_value_sorted['CumulativePercentage'],
             color='red',
             marker='o',
             linewidth=2,
             label='Cumulative Percentage',
             ax=ax2) # Explicitly assign to ax2

# Add horizontal lines for ABC cut-offs
ax2.axhline(y=80, color='gray', linestyle='--', label='80% Cut-off (A-B)')
ax2.axhline(y=95, color='gray', linestyle=':', label='95% Cut-off (B-C)')

# Annotate the cut-off lines (optional, for clearer visualization)
ax2.text(len(brand_sales_value_sorted) * 0.7, 80.5, 'A-B Boundary', color='gray', va='bottom')
ax2.text(len(brand_sales_value_sorted) * 0.7, 95.5, 'B-C Boundary', color='gray', va='bottom')


# Set labels and title
ax1.set_xlabel('Brand (Sorted by Total Sales Value)')
ax1.set_ylabel('Individual Percentage of Total Sales Value (%)', color='skyblue')
ax2.set_ylabel('Cumulative Percentage of Total Sales Value (%)', color='red')
plt.title('ABC Analysis of Brands by Total Sales Value', fontsize=16)

# Set y-axis limits for ax2
ax2.set_ylim(0, 100)

# Rotate x-axis labels for readability if there are many brands
ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90, ha='right', fontsize=6)
# To avoid cluttering the x-axis, we can show labels for only a subset of brands, e.g., every 50th brand
# However, for 7k brands, this plot might be overwhelming even then.
# Let's adjust x-ticks to show only a reasonable number, or maybe none for clarity of the curve.
# For better readability of such a large number of brands, we might hide most x-tick labels.
ax1.set_xticks(ax1.get_xticks()[::100]) # Show label for every 100th brand to avoid extreme clutter

# Combine legends from both axes
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc='center right')

plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Display specific details for each category for a quick summary
a_items = brand_sales_value_sorted[brand_sales_value_sorted['ABC_Category'] == 'A']
b_items = brand_sales_value_sorted[brand_sales_value_sorted['ABC_Category'] == 'B']
c_items = brand_sales_value_sorted[brand_sales_value_sorted['ABC_Category'] == 'C']

print("\n--- Summary of ABC Categories ---")
print(f"A-Items: {len(a_items)} brands, accounting for {a_items['PercentageOfTotalValue'].sum():.2f}% of total sales value.")
print(f"B-Items: {len(b_items)} brands, accounting for {b_items['PercentageOfTotalValue'].sum():.2f}% of total sales value.")
print(f"C-Items: {len(c_items)} brands, accounting for {c_items['PercentageOfTotalValue'].sum():.2f}% of total sales value.")

"""Key Components:

Blue bars: Individual percentage contribution of each brand to total sales

Red line: Cumulative percentage showing running total

Horizontal lines: Category boundaries at 80% (A-B) and 95% (B-C)

Main Insights:
Category A Brands (High Value):

The first few brands (leftmost) contribute disproportionately high sales value
One dominant brand contributes nearly 0.85% individually

These top performers reach 80% of total sales value with relatively few brands

Category B Brands (Medium Value):

Brands between 80-95% cumulative percentage
Moderate individual contributions
Bridge between high and low performers

Category C Brands (Low Value):

The majority of brands (most of the chart)
Very small individual contributions (close to 0%)
Collectively represent only 5% of total sales value

Business Implications:

This classic "long tail" distribution shows that a small number of brands drive the vast majority of sales value, while most brands contribute minimally. This suggests focusing resources and attention on the Category A brands for maximum impact, while considering whether the numerous low-performing Category C brands justify their operational costs.

#TASK 3: Supplier Performance Analysis
"""

print("\n--- Starting Supplier Performance Analysis ---")

# 1. Overall Purchase Volume and Value per Vendor
print("\n1. Overall Purchase Volume and Value per Vendor:")
# Group by 'VendorName' and sum 'Quantity' and 'Dollars' from df_purchases_final_2016
# This gives the total quantity purchased and total value (in dollars) from each vendor.
vendor_purchases = df_purchases_final_2016.groupby('VendorName').agg(
    TotalQuantity=('Quantity', 'sum'),
    TotalValue=('Dollars', 'sum')
).reset_index()

# Sort by TotalValue in descending order to see top vendors by spend.
vendor_purchases_sorted = vendor_purchases.sort_values(by='TotalValue', ascending=False)
print(vendor_purchases_sorted.head(10)) # Display top 10 for brevity

"""Identifies the most significant vendors in terms of spend and quantity. These vendors usually warrant closer relationships, potential for volume discounts, and more rigorous performance monitoring.

A significant portion of your purchasing budget likely goes to a few key suppliers, similar to the ABC analysis of brands.

Insight:

As expected, the purchase volume and value show a very skewed distribution. DIAGEO NORTH AMERICA INC is by far the largest supplier, accounting for a massive share of total purchases ($50.9 million, over double the next closest supplier). This confirms the "Pareto Principle" applies to suppliers as well – a few key suppliers dominate your spending.

Implication:

DIAGEO, MARTIGNETTI, JIM BEAM, PERNOD RICARD, and BACARDI are your strategic suppliers. Their performance directly and significantly impacts your operations and profitability. You should focus on building strong relationships, negotiating favorable terms, and closely monitoring their performance.
"""

# 2. Payment Term Adherence (Days to Pay)
print("\n\n2. Payment Term Adherence (Days to Pay):")
# Calculate the number of days between InvoiceDate and PayDate in both dataframes.
# This gives an indication of how quickly invoices are paid, which can reflect vendor payment terms.

# df_invoice_purchases_2016
df_invoice_purchases_2016['DaysToPay'] = (df_invoice_purchases_2016['PayDate'] - df_invoice_purchases_2016['InvoiceDate']).dt.days
# Filter out negative 'DaysToPay' if they exist (indicating payment before invoice or data error)
df_invoice_purchases_2016_clean_pay = df_invoice_purchases_2016[df_invoice_purchases_2016['DaysToPay'] >= 0]
vendor_payment_terms_invoice = df_invoice_purchases_2016_clean_pay.groupby('VendorName')['DaysToPay'].mean().reset_index()
vendor_payment_terms_invoice.rename(columns={'DaysToPay': 'AvgDaysToPay_Invoice'}, inplace=True)
print("From df_invoice_purchases_2016 (Avg. Days to Pay):")
print(vendor_payment_terms_invoice.head())


# df_purchases_final_2016
df_purchases_final_2016['DaysToPay'] = (df_purchases_final_2016['PayDate'] - df_purchases_final_2016['InvoiceDate']).dt.days
# Filter out negative 'DaysToPay' for consistency
df_purchases_final_2016_clean_pay = df_purchases_final_2016[df_purchases_final_2016['DaysToPay'] >= 0]
vendor_payment_terms_purchases = df_purchases_final_2016_clean_pay.groupby('VendorName')['DaysToPay'].mean().reset_index()
vendor_payment_terms_purchases.rename(columns={'DaysToPay': 'AvgDaysToPay_Purchases'}, inplace=True)
print("\nFrom df_purchases_final_2016 (Avg. Days to Pay):")
print(vendor_payment_terms_purchases.head())

# Merge the payment terms data from both dataframes for a more complete picture.
vendor_payment_terms = pd.merge(vendor_payment_terms_invoice, vendor_payment_terms_purchases, on='VendorName', how='outer')
# For vendors present in both, we might want to take an average or prioritize one source.
# For simplicity, we'll keep both averages for now.
print("\nCombined Payment Term Adherence (Head of merged data):")
print(vendor_payment_terms.head())

"""This metric reflects how quickly your company pays its vendors. A lower average DaysToPay (closer to 0 or within agreed terms) indicates efficient payment processing and good financial health.

Conversely, consistently high DaysToPay might indicate cash flow issues or inefficient payment procedures.

Vendors often have standard payment terms (e.g., Net 30, Net 60). Ideally, our DaysToPay should align with these terms. Significant deviations (very long payment times) could strain vendor relationships.

**Insight:**

The AvgDaysToPay_Invoice and AvgDaysToPay_Purchases are remarkably consistent and hover around 35-37 days for most vendors.

There's a slight variation, with some vendors like AAPER ALCOHOL & CHEMICAL CO showing a slightly higher average (43-44 days).

**Implication:**

This suggests that your payment cycle is fairly consistent, typically settling invoices around 35-37 days after the invoice date. This aligns well with common Net 30 or Net 45 payment terms.

For vendors like AAPER (43-44 days), it could mean they have more extended payment terms, or perhaps there are specific logistical/processing delays for their invoices.
"""

# 3. Delivery Lead Time Performance (Revised)
print("\n3. Delivery Lead Time Performance:")
# Calculate the difference between ReceivingDate and PODate (Order Placement Date).
# This gives the actual lead time for the delivery.
df_purchases_final_2016['ActualDeliveryLeadTime'] = (df_purchases_final_2016['ReceivingDate'] - df_purchases_final_2016['PODate']).dt.days

# Filter out negative lead times (if any, indicating receiving before PO date, which is an anomaly)
df_purchases_final_2016_clean_delivery = df_purchases_final_2016[df_purchases_final_2016['ActualDeliveryLeadTime'] >= 0].copy()


# Group by VendorName and calculate mean, median, and STANDARD DEVIATION of the lead time.
# Standard deviation is key for consistency. Lower std dev means more predictable lead times.
vendor_delivery_performance_revised = df_purchases_final_2016_clean_delivery.groupby('VendorName').agg(
    AvgDeliveryLeadTime=('ActualDeliveryLeadTime', 'mean'),
    MedianDeliveryLeadTime=('ActualDeliveryLeadTime', 'median'),
    StdDevDeliveryLeadTime=('ActualDeliveryLeadTime', 'std'), # New, more useful metric
    MinDeliveryLeadTime=('ActualDeliveryLeadTime', 'min'),    # Added for range understanding
    MaxDeliveryLeadTime=('ActualDeliveryLeadTime', 'max'),    # Added for range understanding
    TotalDeliveriesCount=('ActualDeliveryLeadTime', 'count')
).reset_index()

# Handle cases where StdDev is NaN (e.g., only one delivery for a vendor)
vendor_delivery_performance_revised['StdDevDeliveryLeadTime'].fillna(0, inplace=True)

# Sort by AvgDeliveryLeadTime to see who delivers fastest/slowest on average.
# Then by StdDevDeliveryLeadTime to see who is most consistent.
vendor_delivery_performance_sorted_revised = vendor_delivery_performance_revised.sort_values(
    by=['AvgDeliveryLeadTime', 'StdDevDeliveryLeadTime'], ascending=[True, True]
)

print(vendor_delivery_performance_sorted_revised.head(10)) # Display top 10 consistent vendors

"""**Analysis of Delivery Lead Time Performance:**

Average and Median Delivery Lead Time (AvgDeliveryLeadTime, MedianDeliveryLeadTime):

The average delivery lead time for most vendors seems to cluster around 5 to 8 days.

TRUETT HURST and HIGHLAND WINE MERCHANTS LLC show slightly lower average lead times (5.0 and 5.32 days respectively), indicating faster deliveries on average.

The median lead times are very close to the average lead times for most vendors, suggesting that the distribution of lead times is not heavily skewed by extreme outliers.

Standard Deviation of Delivery Lead Time (StdDevDeliveryLeadTime):

This is the most crucial new metric. It tells us about the consistency and predictability of a vendor's delivery lead times.

TRUETT HURST has a StdDevDeliveryLeadTime of 0.00. This is because they only have 1 TotalDeliveriesCount. With only one data point, the standard deviation is naturally zero, so while it indicates perfect consistency for that single delivery, it doesn't give us enough information about their general reliability.

For other vendors, the standard deviation ranges roughly from 1.97 to 2.72 days. This means that, on average, a vendor's delivery might deviate by about 2-3 days from their average lead time.

MARTIGNETTI COMPANIES (1.97) and Circa Wines (1.98) show relatively lower standard deviations among the vendors with multiple deliveries, suggesting they are more consistent in their delivery times.

ALISA CARR BEVERAGES (2.72) shows a slightly higher standard deviation, indicating their delivery times are a bit more variable.

Min and Max Delivery Lead Time (MinDeliveryLeadTime, MaxDeliveryLeadTime):

These columns provide the range of actual lead times. For most vendors, the minimum lead time is around 3 days and the maximum can go up to 12-14 days.

This range is important for understanding the full spectrum of delivery performance.

For example, while HIGHLAND WINE MERCHANTS LLC has a low average, their deliveries can still take up to 9 days, implying some variability.

Total Deliveries Count (TotalDeliveriesCount):

This column is critical for interpreting the other metrics. A vendor with only 1 delivery (like TRUETT HURST) gives us very little confidence about their true average or consistency.

Vendors with a large number of deliveries (e.g., PHILLIPS PRODUCTS CO. with 4981, MARTIGNETTI COMPANIES with 294) provide a more statistically robust estimate of their performance.

Business Implications of Revised Delivery Metrics:

Inventory Planning & Safety Stock:

Vendors with lower AvgDeliveryLeadTime help reduce the time goods are in transit, potentially lowering overall inventory levels.

Vendors with lower StdDevDeliveryLeadTime (i.e., more predictable deliveries) are invaluable. When lead times are consistent, you need to hold less safety stock to buffer against uncertainty, directly reducing carrying costs and optimizing warehouse space. Conversely, highly variable lead times force you to hold more safety stock, increasing costs.

Supplier Prioritization:

When choosing between suppliers for a specific product, consider not just the price, but also their AvgDeliveryLeadTime and, crucially, their StdDevDeliveryLeadTime. A slightly higher purchase price might be justified by more reliable deliveries and lower safety stock requirements.

For your top "A" and "B" category items (from ABC analysis), you should prioritize suppliers who demonstrate strong consistency in their lead times.

Performance Reviews:

This data provides concrete metrics for supplier performance reviews. You can discuss with vendors like ALISA CARR BEVERAGES (higher standard deviation) ways to improve their consistency.

Negotiations:

This data strengthens your position in negotiations. If a vendor has a high average lead time or high variability, you can factor the associated inventory holding costs or potential stockout risks into your discussions.
"""

# 4. Consolidate Vendor Performance Metrics
print("\n\n4. Consolidated Vendor Performance Metrics:")
# Merge all the dataframes to have a single view of vendor performance.
# Start with overall purchases, then add payment terms, then delivery.
vendor_performance_consolidated = pd.merge(vendor_purchases_sorted, vendor_payment_terms, on='VendorName', how='left')
vendor_performance_consolidated = pd.merge(vendor_performance_consolidated, vendor_delivery_performance_revised[['VendorName', 'AvgDeliveryLeadTime', 'MedianDeliveryLeadTime', 'StdDevDeliveryLeadTime']], on='VendorName', how='left')

# Display the final consolidated vendor performance dataframe.
# Display top 10 by TotalValue, along with their other metrics.
print(vendor_performance_consolidated.head(10))

# Display some basic summary statistics for numerical columns in the consolidated table
print("\n\nSummary Statistics for Consolidated Vendor Performance:")
print(vendor_performance_consolidated.describe().T)

"""**Key Observations from the Top 10 Suppliers:**

Average Lead Time: Most of your top suppliers have remarkably similar average lead times, hovering around 7.5 to 7.9 days. ULTRA BEVERAGE COMPANY LLP stands out slightly higher at 8.41 days.

Lead Time Consistency (Standard Deviation):

The standard deviations are also quite consistent across these major players, generally falling between 2.03 and 2.40 days.

CONSTELLATION BRANDS INC (2.09) and ULTRA BEVERAGE COMPANY LLP (2.03) appear to be slightly more consistent than the others in this top group.

JIM BEAM BRANDS COMPANY (2.40) and E & J GALLO WINERY (2.28) show slightly higher variability, meaning their delivery times are a bit less predictable.

Implications:

Consistency is a Challenge: The standard deviations of ~2-2.5 days are not extremely high but do represent a significant variance. A lead time that can swing by 2-3 days consistently means that for a 7-day average delivery, you could realistically receive it anywhere from 5 to 10 days. This variability directly impacts the amount of safety stock you need to hold to prevent stockouts.

Focus on Key Suppliers: Since your top suppliers account for a huge portion of your spend, even small improvements in their lead time consistency can lead to significant cost savings in inventory carrying costs.

No Obvious 'Bad' Top Performers: Based on these metrics alone, there isn't a single "terrible" top supplier in terms of delivery lead time and consistency.

They are all fairly clustered. However, there's always room for improvement.

#Task 4: Inventory Optimization Strategies (EOQ & Reorder Point)
To calculate EOQ and Reorder Point, we need specific inputs, some of which we've derived in previous tasks, and some for which we'll need to make reasonable assumptions.

Assumptions for EOQ Calculation:
Since we don't have explicit cost data for ordering or holding inventory in the provided datasets, we must make standard assumptions for demonstration purposes. In a real-world scenario, these would be derived from financial records.

Ordering Cost (S): The cost incurred each time an order is placed (e.g., administrative costs, transportation costs, setup costs).

Let's assume S=$100 per order.

Holding Cost Percentage (i): The percentage of an item's value that it costs to hold it in inventory for one year (e.g., warehousing costs, insurance, obsolescence, capital costs).

Let's assume i=20% per year (or 0.20).

Data Needed for EOQ & Reorder Point:

Annual Demand (D):

We'll use Brand 8111 as our example, as it was the top-selling brand selected for demand forecasting.

We have 60 days of sales data for Brand 8111. We will extrapolate this to an annual demand.

Cost Per Unit (C):

We need the average purchase price of Brand 8111. We can derive this from df_purchases_final_2016.

Holding Cost per Unit per Year (H):

H=i×C

Average Daily Demand:

From the DailySalesQuantity of Brand 8111.

Average Lead Time:

We'll need the average delivery lead time for the supplier of Brand 8111 from our Task 3 analysis.

##4.1: Inventory Optimization Strategies (EOQ & Reorder Point)
"""

print("--- Starting Task 4: Inventory Optimization Strategies (EOQ & Reorder Point) ---")

# --- Step 1: Determine the specific Brand and its Supplier ---
# We'll continue with Brand ID 8111 as our example, as it was the top-selling brand.
selected_brand_id = 8111

# To link the brand to a supplier, we need to find which supplier sells Brand 8111.
# This assumes a brand is typically supplied by one primary vendor, or we take an average.
# Let's find the vendor(s) who supplied Brand 8111 in df_purchases_final_2016.
brand_purchases = df_purchases_final_2016[df_purchases_final_2016['Brand'] == selected_brand_id]

if brand_purchases.empty:
    print(f"Warning: No purchase data found for Brand {selected_brand_id}. Cannot proceed with EOQ for this brand.")
    # Fallback: If no purchases, we can't get purchase price or lead time.
    # For demo, we'll use a placeholder and warn.
    avg_purchase_price_brand = 10.0 # Placeholder
    supplier_avg_lead_time = 7 # Placeholder
    primary_supplier_name = "N/A (No purchase data)"
else:
    # Find the primary supplier for Brand 8111 (e.g., the one with the highest total quantity supplied)
    primary_supplier = brand_purchases.groupby('VendorName')['Quantity'].sum().idxmax()
    primary_supplier_name = primary_supplier
    print(f"\nPrimary supplier for Brand {selected_brand_id}: {primary_supplier_name}")

    # --- Step 2: Calculate Annual Demand (D) for Brand 8111 ---
    # Aggregate daily sales for Brand 8111 from df_sales_final_2016
    brand_sales_ts = df_sales_final_2016[df_sales_final_2016['Brand'] == selected_brand_id]
    brand_sales_ts_daily = brand_sales_ts.groupby('SalesDate')['SalesQuantity'].sum().reset_index()
    brand_sales_ts_daily.set_index('SalesDate', inplace=True)

    # Reindex to a full date range to account for days with zero sales
    full_date_range = pd.date_range(start=brand_sales_ts_daily.index.min(),
                                    end=brand_sales_ts_daily.index.max(),
                                    freq='D')
    brand_sales_ts_daily = brand_sales_ts_daily.reindex(full_date_range, fill_value=0)

    # Calculate average daily demand from the available data
    total_sales_days = (brand_sales_ts_daily.index.max() - brand_sales_ts_daily.index.min()).days + 1
    total_sales_quantity = brand_sales_ts_daily['SalesQuantity'].sum()
    avg_daily_demand = total_sales_quantity / total_sales_days

    # Extrapolate to annual demand (assuming 365 operating days for simplicity)
    annual_demand_D = avg_daily_demand * 365
    print(f"Average Daily Demand for Brand {selected_brand_id}: {avg_daily_demand:.2f} units")
    print(f"Estimated Annual Demand (D) for Brand {selected_brand_id}: {annual_demand_D:.2f} units")

    # --- Step 3: Calculate Average Cost Per Unit (C) for Brand 8111 ---
    # Use PurchasePrice from df_purchases_final_2016 for Brand 8111
    avg_purchase_price_brand = brand_purchases['PurchasePrice'].mean()
    if pd.isna(avg_purchase_price_brand):
        print(f"Warning: No valid PurchasePrice found for Brand {selected_brand_id}. Using placeholder $10.0.")
        avg_purchase_price_brand = 10.0

    print(f"Average Cost Per Unit (C) for Brand {selected_brand_id}: ${avg_purchase_price_brand:.2f}")

    # --- Step 4: Get Average Lead Time for the Supplier of Brand 8111 ---
    # Recalculate vendor delivery performance including StdDevDeliveryLeadTime
    df_purchases_final_2016['ActualDeliveryLeadTime'] = (df_purchases_final_2016['ReceivingDate'] - df_purchases_final_2016['PODate']).dt.days
    df_purchases_final_2016_clean_delivery = df_purchases_final_2016[df_purchases_final_2016['ActualDeliveryLeadTime'] >= 0].copy()

    vendor_delivery_performance_revised = df_purchases_final_2016_clean_delivery.groupby('VendorName').agg(
        AvgDeliveryLeadTime=('ActualDeliveryLeadTime', 'mean'),
        StdDevDeliveryLeadTime=('ActualDeliveryLeadTime', 'std'),
        TotalDeliveriesCount=('ActualDeliveryLeadTime', 'count')
    ).reset_index()
    vendor_delivery_performance_revised['StdDevDeliveryLeadTime'].fillna(0, inplace=True)

    supplier_lead_time_info = vendor_delivery_performance_revised[vendor_delivery_performance_revised['VendorName'] == primary_supplier_name]

    if not supplier_lead_time_info.empty:
        supplier_avg_lead_time = supplier_lead_time_info['AvgDeliveryLeadTime'].iloc[0]
        supplier_std_dev_lead_time = supplier_lead_time_info['StdDevDeliveryLeadTime'].iloc[0]
    else:
        print(f"Warning: No delivery lead time data found for supplier {primary_supplier_name}. Using placeholder 7 days.")
        supplier_avg_lead_time = 7 # Placeholder if supplier data is missing
        supplier_std_dev_lead_time = 0 # Placeholder

    print(f"Average Lead Time (L) for {primary_supplier_name}: {supplier_avg_lead_time:.2f} days")
    print(f"Std Dev Lead Time for {primary_supplier_name}: {supplier_std_dev_lead_time:.2f} days")

"""##4.2 EOQ Calculation

The Economic Order Quantity (EOQ) formula is:

EOQ= sqrt(2DS/H)

Where:

D = Annual Demand (units)
S = Ordering Cost per order
H = Holding Cost per unit per year (H=i×C)
"""

# Define assumed costs (as stated above)
ordering_cost_S = 100 # $ per order
holding_cost_percentage_i = 0.20 # 20% of unit cost per year

# Calculate Holding Cost (H) per unit per year
holding_cost_H = holding_cost_percentage_i * avg_purchase_price_brand

if holding_cost_H == 0: # Avoid division by zero if avg_purchase_price_brand was 0 or very small
    print("Error: Holding cost H is zero. Cannot calculate EOQ. Check average purchase price.")
    eoq = 0
else:
    # Calculate EOQ
    eoq = np.sqrt((2 * annual_demand_D * ordering_cost_S) / holding_cost_H)

print(f"\n--- EOQ Calculation for Brand {selected_brand_id} ---")
print(f"Assumed Ordering Cost (S): ${ordering_cost_S:.2f}")
print(f"Assumed Annual Holding Cost Percentage (i): {holding_cost_percentage_i * 100:.0f}%")
print(f"Calculated Holding Cost (H) per unit per year: ${holding_cost_H:.2f}")
print(f"Calculated Economic Order Quantity (EOQ): {eoq:.0f} units")

"""##4.3 Reorder Point Calculation

The Reorder Point (ROP) is the inventory level at which a new order should be placed to avoid stockouts during the lead time.

ROP= (AverageDailyDemand × AverageLeadTime) + SafetyStock

Average Daily Demand: We already calculated this.
Average Lead Time: We calculated this for the supplier.
Safety Stock: This is extra inventory held to protect against uncertainties in demand or lead time. Calculating robust safety stock requires a desired service level and the standard deviation of both demand and lead time.
For a simplified demonstration, let's first calculate ROP without safety stock (just demand during lead time), and then explain how safety stock would be added.

Simplified Reorder Point (without Safety Stock):

ROP_simple=AverageDailyDemand × AverageLeadTime
"""

# Calculate simple Reorder Point
reorder_point_simple = avg_daily_demand * supplier_avg_lead_time

print(f"\n--- Reorder Point Calculation for Brand {selected_brand_id} ---")
print(f"Average Daily Demand: {avg_daily_demand:.2f} units/day")
print(f"Average Lead Time: {supplier_avg_lead_time:.2f} days")
print(f"Simplified Reorder Point (Demand during Lead Time): {reorder_point_simple:.0f} units")

# --- Incorporating Safety Stock (Conceptual Explanation) ---
# To calculate safety stock accurately, we need:
# 1. Service Level (e.g., 95% or 99% - determines Z-score)
# 2. Standard Deviation of Daily Demand (from our demand forecasting)
# 3. Standard Deviation of Lead Time (from supplier performance analysis)

# For demonstration, let's assume a desired service level (e.g., 95%) and a Z-score.
# Z-score for 95% service level is approx. 1.645
# Z-score for 99% service level is approx. 2.33

# Let's use a dummy Z-score for conceptual safety stock
z_score = 1.645 # For 95% service level

# A common formula for safety stock when both demand and lead time are variable:
# Safety Stock = Z * sqrt((AvgLeadTime * StdDevDailyDemand^2) + (AvgDailyDemand^2 * StdDevLeadTime^2))
# This is more complex than typically covered in an initial EOQ demo without dedicated time series features.

# For simplicity, let's use a common approximation if daily demand STD DEV is not explicitly derived from forecast:
# Safety Stock = Z-score * Std Dev of Demand during Lead Time
# Std Dev of Demand during Lead Time = sqrt(AvgLeadTime) * Std Dev Daily Demand (if lead time is constant)
# OR if lead time is variable: sqrt(L * Sigma_d^2 + D_avg^2 * Sigma_L^2)

# For this demo, let's use a simpler approach for safety stock, based on lead time variability
# Safety Stock = Z * StdDevLeadTime * AvgDailyDemand (this is an approximation, a more robust formula considers demand variability too)
# We don't have StdDevDailyDemand directly calculated from Task 1 (we have std dev of sales for the period).
# So, let's just use a simplified conceptual safety stock:
# Safety Stock = Z-score * (std dev of demand * sqrt(lead time)) OR Z-score * (avg daily demand * std dev of lead time)

# Let's calculate the overall standard deviation of sales from our brand_sales_ts_daily
std_dev_daily_demand = brand_sales_ts_daily['SalesQuantity'].std()
if pd.isna(std_dev_daily_demand):
    std_dev_daily_demand = 0 # If only one day of sales or constant sales

# A simpler safety stock calculation
safety_stock_simplified = z_score * std_dev_daily_demand * np.sqrt(supplier_avg_lead_time)

# If supplier_avg_lead_time is zero, sqrt will be zero, or if std_dev_daily_demand is zero.
if pd.isna(safety_stock_simplified) or safety_stock_simplified < 0:
    safety_stock_simplified = 0 # Ensure safety stock is non-negative and not NaN


reorder_point_with_safety_stock = reorder_point_simple + safety_stock_simplified

print(f"Estimated Standard Deviation of Daily Demand: {std_dev_daily_demand:.2f} units")
print(f"Assumed Z-score for 95% service level: {z_score}")
print(f"Calculated Safety Stock (Simplified): {safety_stock_simplified:.0f} units")
print(f"Reorder Point (ROP) with Safety Stock: {reorder_point_with_safety_stock:.0f} units")

"""##4.4 Interpretation and Strategic Implications

Economic Order Quantity (EOQ):

The calculated EOQ is the theoretical optimal quantity to order each time to minimize the combined costs of ordering and holding inventory, given our assumptions.

Implication: Ordering in quantities close to the EOQ helps in efficient inventory management. Ordering much smaller than EOQ increases ordering frequency and costs; ordering much larger increases holding costs.

Limitations: EOQ assumes constant demand, lead time, and costs. It doesn't account for quantity discounts, supplier capacity, or minimum order quantities (MOQs), which are common in real-world scenarios. It's a starting point, not a rigid rule.

Reorder Point (ROP):

The ROP tells you when to place an order. When the inventory level of Brand 8111 drops to the calculated ROP, a new order of EOQ units should be placed.
Safety Stock: The addition of safety stock is crucial. It acts as a buffer against unforeseen increases in demand or delays in lead time.

The higher the variability in demand (std dev of daily demand) or lead time (std dev of lead time from Task 3), the more safety stock is generally required to maintain a given service level.

The choice of Z-score directly reflects the desired service level (e.g., 95% service level means you expect to meet demand 95% of the time without stockouts). A higher service level requires more safety stock.

Implication: By setting an appropriate ROP with safety stock, the company can proactively manage inventory to prevent stockouts and ensure product availability for customers, even with some unpredictability.

#5. Overall Inventory Optimization Strategy:

Demand Forecasting (Task 1): Provides the Annual Demand (D) and Average Daily Demand, which are critical inputs.

ABC Analysis (Task 2): Prioritizes which items (brands) require more rigorous EOQ and ROP calculations. For "A" items, precise calculations and frequent review are crucial. For "C" items, simpler heuristics might suffice.

Supplier Performance Analysis (Task 3): Provides the Average Lead Time and Standard Deviation of Lead Time, which are essential for ROP and safety stock calculations. Reliable suppliers (low lead time variability) allow for lower safety stock.

EOQ & ROP (Task 4): Translate demand, costs, and lead times into actionable inventory policies: how much to order and when to order.
"""

